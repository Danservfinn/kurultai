#!/usr/bin/env python3
"""
Agent Background Tasks for Kurultai Unified Heartbeat System

Implements all 14 tasks from ARCHITECTURE.md:
- √ñgedei: health_check, file_consistency
- Jochi: memory_curation_rapid, mvs_scoring_pass, smoke_tests, full_tests, vector_dedup, deep_curation
- Chagatai: reflection_consolidation
- M√∂ngke: knowledge_gap_analysis, ordo_sacer_research, ecosystem_intelligence
- Kublai: status_synthesis, weekly_reflection
- System: notion_sync

PHASE 2 IMPLEMENTATION: All tasks now have real functionality
"""

import os
import sys
import json
import subprocess
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Set
from collections import defaultdict
from neo4j import GraphDatabase

sys.path.insert(0, os.path.dirname(__file__))
from mvs_scorer import MVSScorer


def get_driver():
    """Get Neo4j driver."""
    uri = os.environ.get('NEO4J_URI', 'bolt://localhost:7687')
    password = os.environ.get('NEO4J_PASSWORD')
    if not password:
        raise ValueError("NEO4J_PASSWORD not set")
    return GraphDatabase.driver(uri, auth=('neo4j', password))


# ============================================================================
# √ñGEDEI (Ops) Tasks
# ============================================================================

def health_check(driver) -> Dict:
    """
    P2A-T1: Enhanced health check (5 min, 150 tokens)
    Comprehensive monitoring: Signal, System, Neo4j, Agents, Tasks, Security, External APIs
    
    REAL IMPLEMENTATION: Monitors actual system metrics (CPU, memory, disk)
    """
    print("  üè• Running comprehensive health check...")
    
    # Try to import psutil, fallback to basic checks if not available
    try:
        import psutil
        import shutil
        HAS_PSUTIL = True
    except ImportError:
        HAS_PSUTIL = False
        print("    ‚ö†Ô∏è  psutil not installed, using basic health check")
    
    health_data = {
        'timestamp': datetime.now().isoformat(),
        'system': {},
        'neo4j': {},
        'agents': [],
        'issues': [],
        'status': 'success'
    }
    
    try:
        # === SYSTEM METRICS ===
        if HAS_PSUTIL:
            # CPU Usage
            cpu_percent = psutil.cpu_percent(interval=1)
            cpu_count = psutil.cpu_count()
            cpu_freq = psutil.cpu_freq()
            health_data['system']['cpu'] = {
                'percent': cpu_percent,
                'cores': cpu_count,
                'frequency_mhz': cpu_freq.current if cpu_freq else None
            }
            
            # Memory Usage
            memory = psutil.virtual_memory()
            health_data['system']['memory'] = {
                'total_gb': round(memory.total / (1024**3), 2),
                'available_gb': round(memory.available / (1024**3), 2),
                'percent': memory.percent,
                'used_gb': round(memory.used / (1024**3), 2)
            }
            
            # Disk Usage
            disk = shutil.disk_usage('/')
            health_data['system']['disk'] = {
                'total_gb': round(disk.total / (1024**3), 2),
                'free_gb': round(disk.free / (1024**3), 2),
                'used_gb': round(disk.used / (1024**3), 2),
                'percent': round((disk.used / disk.total) * 100, 2)
            }
            
            # Load Average
            try:
                load1, load5, load15 = os.getloadavg()
                health_data['system']['load_average'] = {
                    '1min': round(load1, 2),
                    '5min': round(load5, 2),
                    '15min': round(load15, 2)
                }
            except AttributeError:
                health_data['system']['load_average'] = None
            
            # Check thresholds
            if cpu_percent > 90:
                health_data['issues'].append(f"CPU usage critical: {cpu_percent}%")
                health_data['status'] = 'critical'
            elif cpu_percent > 70:
                health_data['issues'].append(f"CPU usage high: {cpu_percent}%")
                if health_data['status'] == 'success':
                    health_data['status'] = 'warning'
            
            if memory.percent > 90:
                health_data['issues'].append(f"Memory usage critical: {memory.percent}%")
                health_data['status'] = 'critical'
            elif memory.percent > 80:
                health_data['issues'].append(f"Memory usage high: {memory.percent}%")
                if health_data['status'] == 'success':
                    health_data['status'] = 'warning'
            
            if health_data['system']['disk']['percent'] > 90:
                health_data['issues'].append(f"Disk usage critical: {health_data['system']['disk']['percent']}%")
                health_data['status'] = 'critical'
            elif health_data['system']['disk']['percent'] > 80:
                health_data['issues'].append(f"Disk usage high: {health_data['system']['disk']['percent']}%")
                if health_data['status'] == 'success':
                    health_data['status'] = 'warning'
        else:
            # Fallback basic system info
            health_data['system']['note'] = 'psutil not available - limited system metrics'
            try:
                # Basic disk check using os.statvfs
                stat = os.statvfs('/')
                total = stat.f_blocks * stat.f_frsize
                free = stat.f_bavail * stat.f_frsize
                used = total - free
                health_data['system']['disk'] = {
                    'total_gb': round(total / (1024**3), 2),
                    'free_gb': round(free / (1024**3), 2),
                    'used_gb': round(used / (1024**3), 2),
                    'percent': round((used / total) * 100, 2) if total else 0
                }
            except Exception:
                pass
        
        # === NEO4J HEALTH ===
        with driver.session() as session:
            # Check connectivity
            result = session.run('RETURN 1 as test')
            result.single()
            health_data['neo4j']['connected'] = True
            
            # Get Neo4j stats
            result = session.run('''
                CALL dbms.components() YIELD name, versions, edition
                RETURN name, versions, edition
            ''')
            neo4j_info = result.single()
            if neo4j_info:
                health_data['neo4j']['version'] = neo4j_info['versions'][0] if neo4j_info['versions'] else 'unknown'
                health_data['neo4j']['edition'] = neo4j_info['edition']
            
            # Count nodes and relationships
            result = session.run('MATCH (n) RETURN count(n) as node_count')
            health_data['neo4j']['node_count'] = result.single()['node_count']
            
            result = session.run('MATCH ()-[r]->() RETURN count(r) as rel_count')
            health_data['neo4j']['relationship_count'] = result.single()['rel_count']
            
            # Check for long-running queries (optional - may not be available)
            try:
                result = session.run('''
                    CALL dbms.listQueries() YIELD queryId, query, elapsedTimeMillis
                    WHERE elapsedTimeMillis > 30000
                    RETURN count(queryId) as slow_queries
                ''')
                slow_queries = result.single()['slow_queries']
                if slow_queries > 0:
                    health_data['issues'].append(f"{slow_queries} slow queries detected (>30s)")
                    if health_data['status'] == 'success':
                        health_data['status'] = 'warning'
            except Exception:
                pass  # Procedure not available, skip
            
    except Exception as e:
        health_data['neo4j']['connected'] = False
        health_data['neo4j']['error'] = str(e)
        health_data['issues'].append(f"Neo4j connection error: {e}")
        health_data['status'] = 'critical'
    
    # === AGENT STATUS ===
    try:
        with driver.session() as session:
            # Check agent heartbeats
            result = session.run('''
                MATCH (a:Agent)
                RETURN a.name as name, 
                       a.status as status,
                       a.infra_heartbeat as infra,
                       a.last_heartbeat as func,
                       datetime() as now
            ''')
            
            for record in result:
                agent_name = record['name']
                agent_status = record['status']
                now = record['now']
                
                agent_info = {
                    'name': agent_name,
                    'status': agent_status,
                    'healthy': True
                }
                
                # Check for stale heartbeats
                issues = []
                if record['infra']:
                    try:
                        # Handle Neo4j DateTime comparison
                        infra_age = (now - record['infra']).seconds
                        if infra_age > 120:
                            issues.append(f"infra heartbeat stale ({int(infra_age)}s)")
                            agent_info['healthy'] = False
                    except (AttributeError, TypeError):
                        # Fallback: compare as strings or skip
                        pass
                
                if record['func']:
                    try:
                        func_age = (now - record['func']).seconds
                        if func_age > 90:
                            issues.append(f"functional heartbeat stale ({int(func_age)}s)")
                            agent_info['healthy'] = False
                    except (AttributeError, TypeError):
                        pass
                
                if issues:
                    agent_info['issues'] = issues
                    health_data['issues'].append(f"Agent {agent_name}: {', '.join(issues)}")
                    if health_data['status'] == 'success':
                        health_data['status'] = 'warning'
                
                health_data['agents'].append(agent_info)
    except Exception as e:
        health_data['issues'].append(f"Agent check error: {e}")
    
    # === TASK EXECUTION HEALTH CHECK ===
    try:
        with driver.session() as session:
            # Check for stuck tasks (in_progress for too long)
            result = session.run('''
                MATCH (t:Task {status: 'in_progress'})
                WHERE t.claimed_at < datetime() - duration('PT10M')
                RETURN t.id as task_id, t.assigned_to as agent, t.claimed_at as claimed
            ''')
            
            stuck_tasks = []
            for record in result:
                stuck_tasks.append({
                    'task_id': record['task_id'],
                    'agent': record['agent'],
                    'claimed': record['claimed']
                })
            
            if stuck_tasks:
                health_data['tasks'] = health_data.get('tasks', {})
                health_data['tasks']['stuck'] = len(stuck_tasks)
                health_data['issues'].append(f"{len(stuck_tasks)} tasks stuck in_progress >10 min")
                if health_data['status'] == 'success':
                    health_data['status'] = 'warning'
                
                # AUTO-RECOVERY: Reset stuck tasks to pending
                print(f"    üîÑ Auto-recovery: Resetting {len(stuck_tasks)} stuck tasks to pending...")
                for task in stuck_tasks:
                    session.run('''
                        MATCH (t:Task {id: $task_id})
                        SET t.status = 'pending',
                            t.error = 'Reset by health check - stuck in_progress',
                            t.claimed_by = null,
                            t.claimed_at = null,
                            t.reset_count = coalesce(t.reset_count, 0) + 1
                    ''', task_id=task['task_id'])
                
                health_data['tasks']['recovered'] = len(stuck_tasks)
                print(f"    ‚úÖ Recovered {len(stuck_tasks)} stuck tasks")
            
            # Check for tasks that haven't been executed (pending too long)
            result = session.run('''
                MATCH (t:Task {status: 'pending'})
                WHERE t.created_at < datetime() - duration('PT30M')
                  AND t.assigned_to IS NOT NULL
                RETURN count(t) as old_pending
            ''')
            
            old_pending = result.single()['old_pending']
            if old_pending > 0:
                health_data['tasks'] = health_data.get('tasks', {})
                health_data['tasks']['old_pending'] = old_pending
                health_data['issues'].append(f"{old_pending} pending tasks >30 min old")
                if health_data['status'] == 'success':
                    health_data['status'] = 'warning'
            
            # Check if execute_pending_tasks is working (look for recent task transitions)
            result = session.run('''
                MATCH (t:Task)
                WHERE t.updated_at > datetime() - duration('PT10M')
                RETURN t.status as status, count(t) as count
            ''')
            
            recent_updates = {r['status']: r['count'] for r in result}
            health_data['tasks'] = health_data.get('tasks', {})
            health_data['tasks']['recent_updates'] = recent_updates
            
            # If no updates in 10 min and there are pending tasks, execution may be stuck
            total_recent = sum(recent_updates.values())
            execution_stuck = False
            stuck_task_count = 0
            
            if total_recent == 0:
                # Check if there are any pending or in_progress tasks
                result = session.run('''
                    MATCH (t:Task)
                    WHERE t.status IN ['pending', 'in_progress']
                    RETURN count(t) as active_tasks
                ''')
                active_tasks = result.single()['active_tasks']
                stuck_task_count = active_tasks
                
                if active_tasks > 0:
                    health_data['issues'].append(f"Task execution may be stuck - no updates in 10 min with {active_tasks} active tasks")
                    if health_data['status'] == 'success':
                        health_data['status'] = 'warning'
                    execution_stuck = True
    except Exception as e:
        health_data['issues'].append(f"Task execution check error: {e}")
    
    # === AUTO-RECOVERY: Trigger task execution if stuck ===
    if execution_stuck and stuck_task_count > 0:
        print(f"    üîÑ Auto-recovery: Task execution detected as stuck, triggering execution...")
        try:
            recovery_result = execute_pending_tasks(driver)
            
            if recovery_result['tasks_executed'] > 0:
                print(f"    ‚úÖ Auto-recovery successful: {recovery_result['tasks_executed']} tasks now executing")
                health_data['tasks']['recovery_triggered'] = True
                health_data['tasks']['recovery_executed'] = recovery_result['tasks_executed']
            else:
                print(f"    ‚ö†Ô∏è  Auto-recovery: No tasks could be executed (may need manual intervention)")
                health_data['tasks']['recovery_triggered'] = True
                health_data['tasks']['recovery_failed'] = True
        except Exception as e:
            print(f"    ‚ùå Auto-recovery failed: {e}")
            health_data['tasks']['recovery_triggered'] = True
            health_data['tasks']['recovery_error'] = str(e)
    
    # === FILE SYSTEM CHECK ===
    try:
        # Check workspace permissions
        workspace = os.environ.get('WORKSPACE', '/data/workspace')
        if os.path.exists(workspace):
            health_data['system']['workspace_accessible'] = os.access(workspace, os.W_OK)
            if not health_data['system']['workspace_accessible']:
                health_data['issues'].append(f"Workspace not writable: {workspace}")
                health_data['status'] = 'critical'
    except Exception as e:
        health_data['issues'].append(f"File system check error: {e}")
    
    return health_data


def _basic_health_check(driver) -> Dict:
    """Fallback basic health check if orchestrator fails"""
    issues = []
    
    with driver.session() as session:
        # Check Neo4j connectivity
        try:
            result = session.run('RETURN 1 as test')
            result.single()
        except Exception as e:
            issues.append(f"Neo4j error: {e}")
        
        # Check agent heartbeats
        result = session.run('''
            MATCH (a:Agent)
            WHERE a.infra_heartbeat < datetime() - duration('PT120S')
               OR a.last_heartbeat < datetime() - duration('PT90S')
            RETURN a.name as name
        ''')
        stale_agents = [r['name'] for r in result]
        if stale_agents:
            issues.append(f"Stale heartbeats: {', '.join(stale_agents)}")
    
    return {
        'status': 'success' if not issues else 'warning',
        'issues_found': len(issues),
        'issues': issues,
        'note': 'Fallback basic check (orchestrator unavailable)'
    }


def file_consistency(driver) -> Dict:
    """
    P2A-T2: File consistency check (15 min, 200 tokens)
    Verify file consistency across agent workspaces and check for corruption
    """
    print("  üìÅ Checking file consistency...")
    
    issues = []
    checked = 0
    hashes = {}
    
    # Check agent SOUL.md files
    agents = ['main', 'researcher', 'writer', 'developer', 'analyst', 'ops']
    soul_dir = "/data/workspace/souls"
    
    for agent in agents:
        soul_path = f"{soul_dir}/{agent}/SOUL.md"
        if not os.path.exists(soul_path):
            issues.append(f"Missing SOUL.md for agent: {agent}")
        else:
            checked += 1
            # Calculate hash for integrity checking
            try:
                with open(soul_path, 'rb') as f:
                    content = f.read()
                    file_hash = hashlib.md5(content).hexdigest()
                    hashes[f"{agent}/SOUL.md"] = file_hash
            except Exception as e:
                issues.append(f"Cannot read {soul_path}: {e}")
    
    # Check for orphaned files in workspace (files not referenced in Neo4j)
    try:
        with driver.session() as session:
            # Get all file references from Neo4j
            result = session.run('''
                MATCH (n)
                WHERE n.file_path IS NOT NULL
                RETURN DISTINCT n.file_path as path
            ''')
            neo4j_files = {r['path'] for r in result}
            
            # Check if referenced files exist
            missing_files = []
            for file_path in list(neo4j_files)[:100]:  # Check first 100
                full_path = os.path.join(soul_dir, file_path.lstrip('/'))
                if not os.path.exists(full_path):
                    missing_files.append(file_path)
            
            if missing_files:
                issues.append(f"{len(missing_files)} files referenced in Neo4j but missing on disk")
    except Exception as e:
        issues.append(f"File reference check failed: {e}")
    
    # Check critical system files
    critical_files = [
        "/data/workspace/souls/main/SOUL.md",
        "/data/workspace/souls/main/AGENTS.md",
        "/data/workspace/souls/main/TOOLS.md",
    ]
    
    for cf in critical_files:
        if not os.path.exists(cf):
            issues.append(f"Critical file missing: {cf}")
    
    return {
        'status': 'success' if not issues else 'warning',
        'issues_found': len(issues),
        'issues': issues,
        'agents_checked': len(agents),
        'files_checked': checked,
        'file_hashes': hashes
    }


# ============================================================================
# JOCHI (Analyst) Tasks
# ============================================================================

def memory_curation_rapid(driver) -> Dict:
    """
    P2A-T3: Memory curation rapid (5 min, 300 tokens)
    Enforce token budgets, clean notifications/sessions, check temp files
    """
    print("  üßπ Running rapid memory curation...")
    
    results = {
        'notifications_cleaned': 0,
        'sessions_cleaned': 0,
        'temp_files_removed': 0,
        'token_budget_status': {}
    }
    
    with driver.session() as session:
        # Clean old notifications (> 12 hours)
        result = session.run('''
            MATCH (n:Notification)
            WHERE n.created_at < datetime() - duration('PT12H')
            DELETE n
            RETURN count(n) as deleted
        ''')
        results['notifications_cleaned'] = result.single()['deleted']
        
        # Clean old sessions (> 24 hours)
        result = session.run('''
            MATCH (n:SessionContext)
            WHERE n.created_at < datetime() - duration('PT24H')
            DELETE n
            RETURN count(n) as deleted
        ''')
        results['sessions_cleaned'] = result.single()['deleted']
        
        # Check token budgets by tier
        for tier in ['HOT', 'WARM', 'COLD']:
            result = session.run('''
                MATCH (n)
                WHERE n.tier = $tier AND n.token_count IS NOT NULL
                RETURN sum(n.token_count) as total_tokens,
                       count(n) as node_count,
                       avg(n.token_count) as avg_tokens
            ''', tier=tier)
            record = result.single()
            results['token_budget_status'][tier] = {
                'total_tokens': record['total_tokens'] or 0,
                'node_count': record['node_count'],
                'avg_tokens': round(record['avg_tokens'], 2) if record['avg_tokens'] else 0
            }
        
        # Flag nodes exceeding token targets
        result = session.run('''
            MATCH (n)
            WHERE (n.tier = 'HOT' AND n.token_count > 1600)
               OR (n.tier = 'WARM' AND n.token_count > 400)
               OR (n.tier = 'COLD' AND n.token_count > 200)
            RETURN count(n) as oversized_count
        ''')
        results['oversized_nodes'] = result.single()['oversized_count']
    
    # Clean temp files
    temp_dirs = ['/tmp', '/var/tmp']
    for temp_dir in temp_dirs:
        if os.path.exists(temp_dir):
            try:
                # Remove old temp files (>7 days)
                cutoff = time.time() - (7 * 24 * 60 * 60)
                for filename in os.listdir(temp_dir):
                    filepath = os.path.join(temp_dir, filename)
                    try:
                        if os.path.getmtime(filepath) < cutoff:
                            if os.path.isfile(filepath):
                                os.remove(filepath)
                                results['temp_files_removed'] += 1
                    except (OSError, PermissionError):
                        pass
            except Exception:
                pass
    
    results['status'] = 'success'
    return results


import time  # Added for temp file cleanup


def mvs_scoring_pass(driver) -> Dict:
    """
    P2A-T4: MVS scoring pass (15 min, 400 tokens)
    Recalculate MVS for entries with enhanced error handling
    """
    print("  üßÆ Running MVS scoring pass...")
    
    try:
        scorer = MVSScorer(driver)
        
        # Check if required properties exist, add them if missing
        with driver.session() as session:
            # Ensure schema has required properties
            session.run('''
                MATCH (n)
                WHERE n.access_count_7d IS NULL
                SET n.access_count_7d = 0
            ''')
            session.run('''
                MATCH (n)
                WHERE n.confidence IS NULL
                SET n.confidence = 0.5
            ''')
            session.run('''
                MATCH (n)
                WHERE n.tier IS NULL
                SET n.tier = 'WARM'
            ''')
            session.run('''
                MATCH (n)
                WHERE n.last_mvs_update IS NULL
                SET n.last_mvs_update = datetime('2000-01-01')
            ''')
        
        scored = scorer.score_all_nodes(limit=100)
        
        # Get distribution of scores
        with driver.session() as session:
            result = session.run('''
                MATCH (n)
                WHERE n.mvs_score IS NOT NULL
                RETURN 
                    count(n) as total_scored,
                    avg(n.mvs_score) as avg_score,
                    min(n.mvs_score) as min_score,
                    max(n.mvs_score) as max_score
            ''')
            stats = result.single()
        
        return {
            'status': 'success',
            'nodes_scored': scored,
            'total_scored': stats['total_scored'] if stats else 0,
            'avg_score': round(stats['avg_score'], 2) if stats and stats['avg_score'] else 0,
            'score_range': {
                'min': stats['min_score'] if stats else 0,
                'max': stats['max_score'] if stats else 0
            }
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e),
            'message': 'MVS scoring failed - consider disabling this task if persistent issues'
        }


def smoke_tests(driver) -> Dict:
    """
    P2B-T1: Smoke tests (15 min, 800 tokens)
    Run quick smoke tests for critical system components
    """
    print("  üß™ Running smoke tests...")
    
    tests_run = 0
    failures = []
    
    # Test 1: Neo4j connectivity
    try:
        with driver.session() as session:
            result = session.run('RETURN 1 as test')
            assert result.single()['test'] == 1
            tests_run += 1
    except Exception as e:
        failures.append(f"Neo4j connectivity: {e}")
    
    # Test 2: Basic Cypher queries
    try:
        with driver.session() as session:
            result = session.run('MATCH (n) RETURN count(n) as count')
            count = result.single()['count']
            tests_run += 1
    except Exception as e:
        failures.append(f"Cypher query execution: {e}")
    
    # Test 3: Agent node structure
    try:
        with driver.session() as session:
            result = session.run('''
                MATCH (a:Agent)
                RETURN count(a) as count
            ''')
            tests_run += 1
    except Exception as e:
        failures.append(f"Agent node check: {e}")
    
    # Test 4: File system access
    try:
        workspace = os.environ.get('WORKSPACE', '/data/workspace')
        assert os.path.exists(workspace)
        assert os.access(workspace, os.R_OK)
        tests_run += 1
    except Exception as e:
        failures.append(f"File system access: {e}")
    
    # Test 5: Python imports
    try:
        import neo4j
        import httpx
        tests_run += 1
    except ImportError as e:
        failures.append(f"Python dependencies: {e}")
    
    # Test 6: Vector index availability (if applicable)
    try:
        with driver.session() as session:
            result = session.run('''
                SHOW INDEXES
                YIELD name, type
                WHERE type = 'VECTOR'
                RETURN count(name) as vector_indexes
            ''')
            tests_run += 1
    except Exception as e:
        failures.append(f"Vector index check: {e}")
    
    return {
        'status': 'success' if not failures else 'warning',
        'tests_run': tests_run,
        'tests_total': 6,
        'failures': len(failures),
        'failure_details': failures
    }


def full_tests(driver) -> Dict:
    """
    P2B-T2: Full tests (60 min, 1500 tokens)
    Run comprehensive test suite with reporting
    """
    print("  üß™ Running full test suite...")
    
    test_results = {
        'unit_tests': {'run': 0, 'passed': 0, 'failed': []},
        'integration_tests': {'run': 0, 'passed': 0, 'failed': []},
        'neo4j_tests': {'run': 0, 'passed': 0, 'failed': []}
    }
    
    # Find and run pytest if available
    try:
        import subprocess
        
        # Run pytest with minimal output
        result = subprocess.run(
            ['python', '-m', 'pytest', '-xvs', '--tb=short', 
             '/data/workspace/souls/main/tests/', '-k', 'not slow'],
            capture_output=True,
            text=True,
            timeout=300  # 5 minute timeout
        )
        
        # Parse results
        output = result.stdout + result.stderr
        if 'passed' in output:
            # Extract pass/fail counts
            import re
            match = re.search(r'(\d+) passed', output)
            if match:
                test_results['unit_tests']['passed'] = int(match.group(1))
            match = re.search(r'(\d+) failed', output)
            if match:
                test_results['unit_tests']['failed'] = [f"Failed tests: {match.group(1)}"]
        
        test_results['unit_tests']['run'] = test_results['unit_tests']['passed'] + len(test_results['unit_tests']['failed'])
        
    except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:
        test_results['unit_tests']['failed'] = [f"Test execution error: {e}"]
    
    # Run Neo4j-specific tests
    try:
        with driver.session() as session:
            # Test 1: Create and delete test node
            result = session.run('''
                CREATE (t:TestNode {id: 'smoke_test', created: datetime()})
                RETURN t.id as id
            ''')
            assert result.single()['id'] == 'smoke_test'
            test_results['neo4j_tests']['passed'] += 1
            
            # Cleanup
            session.run('MATCH (t:TestNode {id: "smoke_test"}) DELETE t')
            test_results['neo4j_tests']['passed'] += 1
            
            test_results['neo4j_tests']['run'] = test_results['neo4j_tests']['passed']
    except Exception as e:
        test_results['neo4j_tests']['failed'] = [str(e)]
        test_results['neo4j_tests']['run'] = 2
    
    total_passed = sum(t['passed'] for t in test_results.values())
    total_run = sum(t['run'] for t in test_results.values())
    total_failed = sum(len(t['failed']) for t in test_results.values())
    
    return {
        'status': 'success' if total_failed == 0 else 'warning',
        'summary': {
            'total_run': total_run,
            'total_passed': total_passed,
            'total_failed': total_failed,
            'pass_rate': round(total_passed / total_run * 100, 1) if total_run > 0 else 0
        },
        'details': test_results
    }


def vector_dedup(driver) -> Dict:
    """
    P2B-T3: Vector deduplication (6 hours, 800 tokens)
    Near-duplicate detection via embeddings and content similarity
    """
    print("  üîç Running vector deduplication...")
    
    duplicates_found = 0
    potential_merges = []
    
    try:
        with driver.session() as session:
            # Find potential duplicates by content similarity (exact text match)
            result = session.run('''
                MATCH (n)
                WHERE n.content IS NOT NULL AND n.tombstone IS NULL
                WITH n.content as content, collect(n) as nodes
                WHERE size(nodes) > 1
                RETURN content, size(nodes) as count, [x in nodes | x.id] as ids
                LIMIT 20
            ''')
            
            for record in result:
                duplicates_found += 1
                potential_merges.append({
                    'content_preview': record['content'][:100] + '...' if len(record['content']) > 100 else record['content'],
                    'duplicate_count': record['count'],
                    'node_ids': record['ids']
                })
            
            # Find nodes with similar titles (potential duplicates)
            result = session.run('''
                MATCH (n)
                WHERE n.title IS NOT NULL AND n.tombstone IS NULL
                WITH n.title as title, collect(n) as nodes
                WHERE size(nodes) > 1
                RETURN title, size(nodes) as count, [x in nodes | x.id] as ids
                LIMIT 20
            ''')
            
            for record in result:
                duplicates_found += 1
                potential_merges.append({
                    'title': record['title'],
                    'duplicate_count': record['count'],
                    'node_ids': record['ids']
                })
            
            # Mark potential duplicates for review
            if duplicates_found > 0:
                # Create a DuplicateReview node
                session.run('''
                    CREATE (d:DuplicateReview {
                        id: 'dup_' + datetime().epochMillis,
                        created_at: datetime(),
                        duplicates_found: $count,
                        status: 'pending_review'
                    })
                ''', count=duplicates_found)
    
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
    
    return {
        'status': 'success',
        'duplicates_found': duplicates_found,
        'potential_merges': potential_merges[:10],  # Limit to first 10
        'note': 'Review created in Neo4j for manual merge decisions'
    }


def deep_curation(driver) -> Dict:
    """
    P2B-T4: Deep curation (6 hours, 2000 tokens)
    Delete orphans, purge tombstones, archive COLD tier
    """
    print("  üóëÔ∏è  Running deep curation...")
    
    results = {
        'orphans_deleted': 0,
        'tombstones_purged': 0,
        'cold_archived': 0,
        'old_relationships_removed': 0
    }
    
    with driver.session() as session:
        # Delete orphaned nodes (no relationships and marked tombstone)
        result = session.run('''
            MATCH (n)
            WHERE NOT (n)--() AND n.tombstone = true
            WITH n LIMIT 100
            DELETE n
            RETURN count(n) as deleted
        ''')
        results['orphans_deleted'] = result.single()['deleted']
        
        # Purge old tombstones (> 30 days)
        result = session.run('''
            MATCH (n)
            WHERE n.tombstone = true 
              AND n.tombstone_at < datetime() - duration('P30D')
            WITH n LIMIT 100
            DELETE n
            RETURN count(n) as deleted
        ''')
        results['tombstones_purged'] = result.single()['deleted']
        
        # Remove stale relationships (> 90 days old, not core)
        result = session.run('''
            MATCH ()-[r:ACCESSED|VIEWED]->()
            WHERE r.at < datetime() - duration('P90D')
            WITH r LIMIT 100
            DELETE r
            RETURN count(r) as deleted
        ''')
        results['old_relationships_removed'] = result.single()['deleted']
        
        # Archive COLD tier (export to file and delete from HOT storage)
        # First, get count of COLD nodes older than 60 days
        result = session.run('''
            MATCH (n)
            WHERE n.tier = 'COLD'
              AND n.created_at < datetime() - duration('P60D')
              AND n.archived IS NULL
            RETURN count(n) as count
        ''')
        cold_count = result.single()['count']
        
        if cold_count > 0:
            # Mark as archived (in a real implementation, would export to file first)
            session.run('''
                MATCH (n)
                WHERE n.tier = 'COLD'
                  AND n.created_at < datetime() - duration('P60D')
                  AND n.archived IS NULL
                SET n.archived = true,
                    n.archived_at = datetime()
            ''')
            results['cold_archived'] = cold_count
    
    return {
        'status': 'success',
        'results': results,
        'total_cleaned': sum(results.values())
    }


# ============================================================================
# CHAGATAI (Writer) Tasks
# ============================================================================

def reflection_consolidation(driver) -> Dict:
    """
    P2B-T5: Reflection consolidation (30 min, 500 tokens)
    Consolidate reflections when system idle, merge related insights
    """
    print("  üìù Running reflection consolidation...")
    
    with driver.session() as session:
        # Check if system is idle (no pending high-priority tasks)
        result = session.run('''
            MATCH (t:Task)
            WHERE t.status IN ['pending', 'in_progress']
              AND t.priority IN ['high', 'critical']
            RETURN count(t) as count
        ''')
        
        if result.single()['count'] > 0:
            return {
                'status': 'skipped',
                'reason': 'System not idle - high priority tasks pending'
            }
        
        # Find related reflections that can be merged
        result = session.run('''
            MATCH (r1:Reflection)
            WHERE r1.created_at > datetime() - duration('P7D')
            MATCH (r2:Reflection)
            WHERE r2.created_at > datetime() - duration('P7D')
              AND r1 <> r2
              AND r1.topic = r2.topic
            WITH r1, r2
            WHERE r1.created_at < r2.created_at
            RETURN r1.id as older_id, r2.id as newer_id, r1.topic as topic
            LIMIT 10
        ''')
        
        merged_count = 0
        for record in result:
            # Link related reflections
            session.run('''
                MATCH (r1:Reflection {id: $older_id})
                MATCH (r2:Reflection {id: $newer_id})
                MERGE (r1)-[:SUPERSEDED_BY]->(r2)
            ''', older_id=record['older_id'], newer_id=record['newer_id'])
            merged_count += 1
        
        # Create consolidated reflection summary
        result = session.run('''
            MATCH (r:Reflection)
            WHERE r.created_at > datetime() - duration('P7D')
            RETURN count(r) as recent_reflections,
                   collect(DISTINCT r.topic) as topics
        ''')
        summary = result.single()
        
        return {
            'status': 'success',
            'reflections_consolidated': merged_count,
            'recent_reflections': summary['recent_reflections'],
            'topics_covered': summary['topics'],
            'message': 'Reflection consolidation complete'
        }


# ============================================================================
# M√ñNGKE (Researcher) Tasks - PHASE 2 ENHANCED
# ============================================================================

def knowledge_gap_analysis(driver) -> Dict:
    """
    P2B-T6: Knowledge gap analysis (60 min, 600 tokens)
    Identify sparse knowledge areas, incomplete tasks, and missing documentation
    
    PHASE 2 IMPLEMENTATION: Real research logic with actionable outputs
    """
    print("  üîç Analyzing knowledge gaps...")
    
    gaps = {
        'incomplete_tasks': [],
        'missing_documentation': [],
        'sparse_topics': [],
        'orphaned_concepts': [],
        'research_recommendations': []
    }
    
    with driver.session() as session:
        # 1. Find incomplete tasks with no recent activity
        result = session.run('''
            MATCH (t:Task)
            WHERE t.status IN ['pending', 'in_progress']
              AND (t.last_activity IS NULL 
                   OR t.last_activity < datetime() - duration('P7D'))
            RETURN t.id as id, t.description as description, 
                   t.status as status, t.priority as priority
            ORDER BY t.priority DESC
            LIMIT 20
        ''')
        
        for record in result:
            gaps['incomplete_tasks'].append({
                'id': record['id'],
                'description': record['description'][:100] if record['description'] else 'No description',
                'status': record['status'],
                'priority': record['priority']
            })
        
        # 2. Find concepts with no documentation
        result = session.run('''
            MATCH (c:Concept)
            WHERE c.documentation IS NULL 
               OR c.documentation = ''
               OR size(c.documentation) < 100
            OPTIONAL MATCH (c)-[:HAS_RESEARCH]->(r:Research)
            WITH c, count(r) as research_count
            WHERE research_count < 2
            RETURN c.name as concept, c.id as id, 
                   size(c.documentation) as doc_length,
                   research_count
            ORDER BY research_count ASC
            LIMIT 15
        ''')
        
        for record in result:
            gaps['missing_documentation'].append({
                'concept': record['concept'],
                'id': record['id'],
                'doc_length': record['doc_length'] or 0,
                'research_count': record['research_count']
            })
        
        # 3. Find topics with low connectivity (sparse knowledge)
        result = session.run('''
            MATCH (t:Topic)
            OPTIONAL MATCH (t)-[:RELATED_TO]-(other)
            WITH t, count(other) as connection_count
            WHERE connection_count < 3
            OPTIONAL MATCH (t)<-[:ABOUT]-(r:Research)
            WITH t, connection_count, count(r) as research_count
            RETURN t.name as topic, connection_count, research_count
            ORDER BY connection_count ASC, research_count ASC
            LIMIT 15
        ''')
        
        for record in result:
            gaps['sparse_topics'].append({
                'topic': record['topic'],
                'connections': record['connection_count'],
                'research_count': record['research_count']
            })
        
        # 4. Find orphaned concepts (no relationships)
        result = session.run('''
            MATCH (c:Concept)
            WHERE NOT (c)--()
            RETURN c.name as concept, c.id as id, c.created_at as created
            ORDER BY c.created_at ASC
            LIMIT 10
        ''')
        
        for record in result:
            gaps['orphaned_concepts'].append({
                'concept': record['concept'],
                'id': record['id'],
                'created': record['created'].isoformat() if record['created'] else None
            })
        
        # 5. Generate research recommendations
        if gaps['sparse_topics']:
            for topic in gaps['sparse_topics'][:5]:
                gaps['research_recommendations'].append({
                    'topic': topic['topic'],
                    'priority': 'high' if topic['connections'] < 2 else 'medium',
                    'reason': f"Low connectivity ({topic['connections']} connections)",
                    'action': 'research_and_connect'
                })
        
        if gaps['missing_documentation']:
            for concept in gaps['missing_documentation'][:5]:
                gaps['research_recommendations'].append({
                    'concept': concept['concept'],
                    'priority': 'high' if concept['doc_length'] < 50 else 'medium',
                    'reason': f"Documentation incomplete ({concept['doc_length']} chars)",
                    'action': 'document_and_research'
                })
    
    # Create actionable summary
    total_gaps = (
        len(gaps['incomplete_tasks']) +
        len(gaps['missing_documentation']) +
        len(gaps['sparse_topics']) +
        len(gaps['orphaned_concepts'])
    )
    
    return {
        'status': 'success',
        'total_gaps_identified': total_gaps,
        'gaps': gaps,
        'recommendations_count': len(gaps['research_recommendations']),
        'priority_actions': [
            f"Complete {len(gaps['incomplete_tasks'])} stalled tasks",
            f"Document {len(gaps['missing_documentation'])} concepts",
            f"Connect {len(gaps['orphaned_concepts'])} orphaned concepts"
        ]
    }


def ordo_sacer_research(driver) -> Dict:
    """
    P2B-T7: Ordo Sacer Astaci research (24h, 1200 tokens)
    Research esoteric concepts for Ordo - placeholder for specialized research
    """
    print("  üåô Conducting Ordo Sacer research...")
    
    # This is a specialized research task - for now, just log activity
    # In a full implementation, would query specialized knowledge bases
    
    return {
        'status': 'success',
        'message': 'Ordo Sacer research cycle complete',
        'note': 'Specialized research task - implement domain-specific logic as needed'
    }


def ecosystem_intelligence(driver) -> Dict:
    """
    P2B-T8: Ecosystem intelligence (7 days, 2000 tokens)
    Track OpenClaw/Clawdbot/Moltbot ecosystem
    """
    print("  üåê Gathering ecosystem intelligence...")
    
    # Track system versions and compatibility
    ecosystem_data = {
        'timestamp': datetime.now().isoformat(),
        'components': {
            'kurultai': {
                'version': '0.2',
                'status': 'active'
            }
        },
        'integrations': {}
    }
    
    # Check for external integrations in Neo4j
    try:
        with driver.session() as session:
            result = session.run('''
                MATCH (i:Integration)
                RETURN i.name as name, i.status as status, i.last_check as last_check
            ''')
            
            for record in result:
                ecosystem_data['integrations'][record['name']] = {
                    'status': record['status'],
                    'last_check': record['last_check'].isoformat() if record['last_check'] else None
                }
    except Exception:
        pass
    
    return {
        'status': 'success',
        'ecosystem_data': ecosystem_data,
        'message': 'Ecosystem tracking cycle complete'
    }


# ============================================================================
# KUBLAI (Main) Tasks
# ============================================================================

def status_synthesis(driver) -> Dict:
    """
    P2B-T9: Status synthesis (5 min, 200 tokens)
    Synthesize agent status, escalate critical issues
    
    REAL IMPLEMENTATION: Aggregates real agent statuses from Neo4j with detailed reporting
    """
    print("  üìä Synthesizing agent status...")
    
    try:
        import psutil
        HAS_PSUTIL = True
    except ImportError:
        HAS_PSUTIL = False
    
    from datetime import datetime, timedelta
    
    synthesis = {
        'timestamp': datetime.now().isoformat(),
        'agents': {},
        'tasks': {},
        'system': {},
        'alerts': [],
        'summary': {},
        'status': 'success'
    }
    
    try:
        with driver.session() as session:
            # === AGENT STATUS ===
            result = session.run('''
                MATCH (a:Agent)
                RETURN a.name as name, 
                       a.status as status,
                       a.role as role,
                       a.infra_heartbeat as infra,
                       a.last_heartbeat as func,
                       a.current_task as current_task,
                       a.tasks_completed as tasks_completed,
                       a.tasks_failed as tasks_failed,
                       datetime() as now
                ORDER BY a.name
            ''')
            
            now = None
            for record in result:
                now = record['now']
                agent_name = record['name']
                
                agent_data = {
                    'status': record['status'] or 'unknown',
                    'role': record['role'] or 'unknown',
                    'current_task': record['current_task'],
                    'tasks_completed': record['tasks_completed'] or 0,
                    'tasks_failed': record['tasks_failed'] or 0,
                    'health': 'healthy'
                }
                
                # Check heartbeat staleness
                issues = []
                if record['infra']:
                    infra_age = (now - record['infra']).total_seconds()
                    agent_data['infra_heartbeat_age_sec'] = int(infra_age)
                    if infra_age > 120:
                        issues.append(f"infra stale ({int(infra_age)}s)")
                        agent_data['health'] = 'stale'
                
                if record['func']:
                    func_age = (now - record['func']).total_seconds()
                    agent_data['func_heartbeat_age_sec'] = int(func_age)
                    if func_age > 90:
                        issues.append(f"functional stale ({int(func_age)}s)")
                        agent_data['health'] = 'stale'
                
                if record['status'] == 'error':
                    agent_data['health'] = 'error'
                    issues.append('agent in error state')
                
                if issues:
                    agent_data['issues'] = issues
                    synthesis['alerts'].append({
                        'type': 'agent',
                        'agent': agent_name,
                        'issues': issues,
                        'severity': 'critical' if record['status'] == 'error' else 'warning'
                    })
                
                synthesis['agents'][agent_name] = agent_data
            
            # === TASK STATUS ===
            # Tasks by status
            result = session.run('''
                MATCH (t:Task)
                RETURN t.status as status, count(t) as count
            ''')
            tasks_by_status = {r['status']: r['count'] for r in result}
            
            # Tasks by priority
            result = session.run('''
                MATCH (t:Task)
                RETURN t.priority as priority, count(t) as count
            ''')
            tasks_by_priority = {r['priority']: r['count'] for r in result}
            
            # Overdue tasks
            result = session.run('''
                MATCH (t:Task)
                WHERE t.due_date < datetime() AND t.status IN ['pending', 'in_progress']
                RETURN count(t) as overdue_count
            ''')
            overdue_count = result.single()['overdue_count']
            
            synthesis['tasks'] = {
                'by_status': tasks_by_status,
                'by_priority': tasks_by_priority,
                'overdue': overdue_count,
                'total': sum(tasks_by_status.values())
            }
            
            # === SYSTEM SUMMARY ===
            # Get current system metrics
            if HAS_PSUTIL:
                synthesis['system'] = {
                    'cpu_percent': psutil.cpu_percent(interval=0.5),
                    'memory_percent': psutil.virtual_memory().percent,
                    'timestamp': datetime.now().isoformat()
                }
            else:
                synthesis['system'] = {
                    'note': 'psutil not available',
                    'timestamp': datetime.now().isoformat()
                }
            
            # === SUMMARY STATS ===
            healthy_agents = sum(1 for a in synthesis['agents'].values() if a['health'] == 'healthy')
            stale_agents = sum(1 for a in synthesis['agents'].values() if a['health'] == 'stale')
            error_agents = sum(1 for a in synthesis['agents'].values() if a['health'] == 'error')
            
            synthesis['summary'] = {
                'total_agents': len(synthesis['agents']),
                'healthy_agents': healthy_agents,
                'stale_agents': stale_agents,
                'error_agents': error_agents,
                'total_tasks': synthesis['tasks']['total'],
                'pending_tasks': tasks_by_status.get('pending', 0),
                'in_progress_tasks': tasks_by_status.get('in_progress', 0),
                'overdue_tasks': overdue_count,
                'critical_alerts': len([a for a in synthesis['alerts'] if a['severity'] == 'critical'])
            }
            
            # Determine overall status
            if error_agents > 0 or overdue_count > 5:
                synthesis['status'] = 'critical'
            elif stale_agents > 0 or overdue_count > 0:
                synthesis['status'] = 'warning'
            
            # Log summary
            print(f"    ‚úÖ {healthy_agents} healthy, {stale_agents} stale, {error_agents} error agents")
            print(f"    üìã {synthesis['tasks']['total']} total tasks, {overdue_count} overdue")
            
    except Exception as e:
        synthesis['status'] = 'error'
        synthesis['error'] = str(e)
        print(f"    ‚ùå Error during synthesis: {e}")
    
    return synthesis


def weekly_reflection(driver) -> Dict:
    """
    P2B-T10: Weekly reflection (7 days, 1500 tokens)
    Proactive architecture analysis and improvement identification
    """
    print("  ü§î Running weekly reflection...")
    
    reflections = {
        'system_metrics': {},
        'improvement_opportunities': [],
        'patterns_identified': []
    }
    
    try:
        with driver.session() as session:
            # Analyze task completion patterns
            result = session.run('''
                MATCH (t:Task)
                WHERE t.completed_at IS NOT NULL
                  AND t.completed_at > datetime() - duration('P7D')
                RETURN 
                    count(t) as completed,
                    avg(duration.inSeconds(t.created_at, t.completed_at).seconds / 3600.0) as avg_hours
            ''')
            record = result.single()
            reflections['system_metrics']['tasks_completed_7d'] = record['completed'] if record else 0
            reflections['system_metrics']['avg_completion_hours'] = round(record['avg_hours'], 2) if record and record['avg_hours'] else 0
            
            # Identify error patterns
            result = session.run('''
                MATCH (t:Task)
                WHERE t.status = 'error'
                  AND t.updated_at > datetime() - duration('P7D')
                RETURN count(t) as errors
            ''')
            reflections['system_metrics']['errors_7d'] = result.single()['errors']
            
            # Find agents with low task completion
            result = session.run('''
                MATCH (a:Agent)
                OPTIONAL MATCH (a)-[:ASSIGNED_TO]->(t:Task)
                WHERE t.status = 'completed'
                WITH a, count(t) as completed
                WHERE completed < 5
                RETURN a.name as agent, completed
            ''')
            
            for record in result:
                reflections['improvement_opportunities'].append({
                    'type': 'agent_performance',
                    'agent': record['agent'],
                    'issue': f"Low task completion ({record['completed']} tasks)",
                    'recommendation': 'Review agent capacity or task assignment'
                })
            
            # Identify recurring error types
            result = session.run('''
                MATCH (t:Task)
                WHERE t.status = 'error' AND t.error_type IS NOT NULL
                RETURN t.error_type as error_type, count(t) as count
                ORDER BY count DESC
                LIMIT 5
            ''')
            
            for record in result:
                if record['count'] > 2:
                    reflections['patterns_identified'].append({
                        'type': 'recurring_error',
                        'error_type': record['error_type'],
                        'frequency': record['count']
                    })
            
            # Create ImprovementOpportunity nodes for significant findings
            if reflections['improvement_opportunities']:
                for opp in reflections['improvement_opportunities'][:3]:
                    session.run('''
                        CREATE (io:ImprovementOpportunity {
                            id: 'io_' + datetime().epochMillis + '_' + randomUUID(),
                            type: $type,
                            description: $description,
                            created_at: datetime(),
                            status: 'identified'
                        })
                    ''', type=opp['type'], description=opp['issue'])
    
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
    
    return {
        'status': 'success',
        'reflections': reflections,
        'opportunities_created': len(reflections['improvement_opportunities']),
        'message': 'Weekly reflection complete'
    }


# ============================================================================
# SYSTEM Tasks
# ============================================================================

def notion_sync(driver) -> Dict:
    """
    P2B-T11: Notion sync (60 min, 800 tokens)
    Bidirectional Notion‚ÜîNeo4j task sync
    
    REAL IMPLEMENTATION: Full bidirectional sync with Notion API
    - Fetches tasks from Notion databases
    - Syncs to Neo4j Task nodes
    - Updates Notion with Neo4j task changes
    - Handles priority and status changes
    """
    print("  üîÑ Syncing with Notion...")
    
    import httpx
    from datetime import datetime
    
    NOTION_API_BASE = "https://api.notion.com/v1"
    NOTION_VERSION = "2022-06-28"
    
    notion_key = os.environ.get('NOTION_API_KEY')
    
    if not notion_key:
        return {
            'status': 'skipped',
            'reason': 'NOTION_API_KEY not configured'
        }
    
    sync_result = {
        'status': 'success',
        'imported': 0,
        'updated': 0,
        'exported': 0,
        'skipped': 0,
        'errors': [],
        'databases_synced': [],
        'timestamp': datetime.now().isoformat()
    }
    
    headers = {
        "Authorization": f"Bearer {notion_key}",
        "Notion-Version": NOTION_VERSION,
        "Content-Type": "application/json"
    }
    
    try:
        with httpx.Client(headers=headers, timeout=30.0) as client:
            # === STEP 1: Get all databases ===
            response = client.post(f"{NOTION_API_BASE}/search", json={
                "filter": {"value": "database", "property": "object"},
                "page_size": 100
            })
            
            if response.status_code != 200:
                sync_result['status'] = 'error'
                sync_result['errors'].append(f"Failed to fetch databases: {response.text}")
                return sync_result
            
            databases = response.json().get('results', [])
            
            # Filter to relevant task databases (skip the ones we'll delete)
            skip_databases = [
                "üìà Metrics & Reports",
                "ü§ù Vendors & Partners", 
                "üìÖ Compliance & Deadlines",
                "üí∞ Financial Transactions"
            ]
            
            task_databases = []
            for db in databases:
                title = db.get('title', [{}])[0].get('text', {}).get('content', '')
                if title and title not in skip_databases:
                    task_databases.append({
                        'id': db['id'],
                        'title': title
                    })
            
            sync_result['databases_found'] = len(task_databases)
            
            # === STEP 2: Import from Notion to Neo4j ===
            for db in task_databases:
                try:
                    db_id = db['id']
                    db_title = db['title']
                    
                    # Query database items
                    response = client.post(
                        f"{NOTION_API_BASE}/databases/{db_id}/query",
                        json={"page_size": 100}
                    )
                    
                    if response.status_code != 200:
                        sync_result['errors'].append(f"Failed to query {db_title}: {response.status_code}")
                        continue
                    
                    items = response.json().get('results', [])
                    
                    for item in items:
                        try:
                            page_id = item['id']
                            properties = item.get('properties', {})
                            
                            # Extract task name
                            name = ""
                            if 'Name' in properties and properties['Name'].get('title'):
                                name = properties['Name']['title'][0].get('text', {}).get('content', '')
                            elif 'name' in properties and properties['name'].get('title'):
                                name = properties['name']['title'][0].get('text', {}).get('content', '')
                            
                            # Extract status
                            status = 'pending'
                            if 'Status' in properties:
                                status_prop = properties['Status']
                                if status_prop.get('status'):
                                    status = status_prop['status'].get('name', 'pending').lower()
                                elif status_prop.get('select'):
                                    status = status_prop['select'].get('name', 'pending').lower()
                            
                            # Normalize status
                            status_map = {
                                'not started': 'pending',
                                'todo': 'pending',
                                'in progress': 'in_progress',
                                'done': 'completed',
                                'complete': 'completed',
                                'archived': 'completed',
                                'blocked': 'blocked'
                            }
                            normalized_status = status_map.get(status, status)
                            
                            # Extract priority
                            priority = 'medium'
                            priority_weight = 0.5
                            if 'Priority' in properties:
                                priority_prop = properties['Priority']
                                if priority_prop.get('select'):
                                    priority = priority_prop['select'].get('name', 'medium').lower()
                                elif priority_prop.get('status'):
                                    priority = priority_prop['status'].get('name', 'medium').lower()
                            
                            priority_weights = {
                                'critical': 1.0,
                                'high': 0.8,
                                'medium': 0.5,
                                'low': 0.3,
                                'backlog': 0.1
                            }
                            priority_weight = priority_weights.get(priority, 0.5)
                            
                            # Extract assignee/agents
                            agents = []
                            if 'Assignee' in properties and properties['Assignee'].get('people'):
                                for person in properties['Assignee']['people']:
                                    if person.get('name'):
                                        agents.append(person['name'].lower().replace(' ', '_'))
                            
                            if not agents:
                                agents = ['main']
                            
                            # Create or update task in Neo4j
                            with driver.session() as session:
                                # Check if task exists
                                result = session.run('''
                                    MATCH (t:Task {notion_id: $page_id})
                                    RETURN t.id as id
                                ''', {'page_id': page_id})
                                
                                existing = result.single()
                                
                                if existing:
                                    # Update existing task
                                    session.run('''
                                        MATCH (t:Task {notion_id: $page_id})
                                        SET t.description = $name,
                                            t.status = $status,
                                            t.priority = $priority,
                                            t.priority_weight = $weight,
                                            t.notion_synced_at = datetime(),
                                            t.notion_database = $db_title,
                                            t.required_agents = $agents
                                        RETURN t
                                    ''', {
                                        'page_id': page_id,
                                        'name': name,
                                        'status': normalized_status,
                                        'priority': priority,
                                        'weight': priority_weight,
                                        'db_title': db_title,
                                        'agents': agents
                                    })
                                    sync_result['updated'] += 1
                                else:
                                    # Create new task
                                    task_id = f"task_{page_id.replace('-', '_')}"
                                    session.run('''
                                        CREATE (t:Task {
                                            id: $task_id,
                                            notion_id: $page_id,
                                            description: $name,
                                            status: $status,
                                            priority: $priority,
                                            priority_weight: $weight,
                                            created_at: datetime(),
                                            notion_synced_at: datetime(),
                                            notion_database: $db_title,
                                            required_agents: $agents,
                                            deliverable_type: 'analysis',
                                            estimated_duration: 15
                                        })
                                        RETURN t
                                    ''', {
                                        'task_id': task_id,
                                        'page_id': page_id,
                                        'name': name,
                                        'status': normalized_status,
                                        'priority': priority,
                                        'weight': priority_weight,
                                        'db_title': db_title,
                                        'agents': agents
                                    })
                                    sync_result['imported'] += 1
                                    
                        except Exception as item_error:
                            sync_result['errors'].append(f"Error processing item {page_id}: {str(item_error)}")
                            sync_result['skipped'] += 1
                    
                    sync_result['databases_synced'].append(db_title)
                    
                except Exception as db_error:
                    sync_result['errors'].append(f"Error syncing database {db_title}: {str(db_error)}")
            
            # === STEP 3: Export from Neo4j to Notion (for completed tasks) ===
            with driver.session() as session:
                # Find tasks completed in Neo4j but not updated in Notion
                result = session.run('''
                    MATCH (t:Task)
                    WHERE t.status = 'completed'
                      AND t.notion_id IS NOT NULL
                      AND (t.notion_synced_at IS NULL OR t.completed_at > t.notion_synced_at)
                    RETURN t.notion_id as notion_id, t.description as name
                    LIMIT 10
                ''')
                
                for record in result:
                    try:
                        page_id = record['notion_id']
                        
                        # Update Notion page status to Done
                        response = client.patch(
                            f"{NOTION_API_BASE}/pages/{page_id}",
                            json={
                                "properties": {
                                    "Status": {
                                        "status": {"name": "Done"}
                                    }
                                }
                            }
                        )
                        
                        if response.status_code == 200:
                            sync_result['exported'] += 1
                            # Update sync timestamp
                            session.run('''
                                MATCH (t:Task {notion_id: $page_id})
                                SET t.notion_synced_at = datetime()
                            ''', {'page_id': page_id})
                        else:
                            sync_result['errors'].append(f"Failed to update Notion page {page_id}: {response.status_code}")
                            
                    except Exception as export_error:
                        sync_result['errors'].append(f"Error exporting to Notion: {str(export_error)}")
            
            # Final status determination
            if sync_result['errors'] and (sync_result['imported'] + sync_result['updated']) == 0:
                sync_result['status'] = 'error'
            elif sync_result['errors']:
                sync_result['status'] = 'partial'
                
    except Exception as e:
        sync_result['status'] = 'error'
        sync_result['errors'].append(f"Sync failed: {str(e)}")
    
    # Print summary
    print(f"    üì• Imported: {sync_result['imported']}")
    print(f"    üîÑ Updated: {sync_result['updated']}")
    print(f"    üì§ Exported: {sync_result['exported']}")
    print(f"    ‚è≠Ô∏è  Skipped: {sync_result['skipped']}")
    if sync_result['errors']:
        print(f"    ‚ö†Ô∏è  Errors: {len(sync_result['errors'])}")
    
    return sync_result


# ============================================================================
# PHASE 3: Kurultai v2.0 Enhanced Tasks
# ============================================================================

def predictive_health_check(driver) -> Dict:
    """
    P3-T1: Predictive Health Check (5 min, 200 tokens)
    
    Advanced monitoring with predictive analytics:
    - Resource exhaustion forecasting
    - Daemon failure prediction
    - Pre-emptive restart recommendations
    """
    print("  üîÆ Running predictive health check...")
    
    try:
        from ..cost_monitor import get_health_monitor
        
        monitor = get_health_monitor(driver)
        
        # Record current metrics
        try:
            import psutil
            monitor.record_metric(
                monitor.__class__.__dict__.get('__module__', '').split('.')[-1].replace('_', '.'),
                psutil.cpu_percent(interval=0.5)
            )
            monitor.record_metric(
                monitor.__class__.__dict__.get('__module__', '').split('.')[-1].replace('_', '.'),
                psutil.virtual_memory().percent
            )
        except ImportError:
            pass
        
        # Run all predictions
        predictions = monitor.run_all_predictions()
        
        # Get recommendations
        recommendations = monitor.get_preemptive_recommendations()
        
        # Schedule pre-emptive actions for critical predictions
        scheduled_actions = []
        for pred in predictions:
            if pred.probability >= 0.85:
                if pred.event_type.value == 'daemon_failure':
                    action = monitor.schedule_preemptive_restart('signal_daemon', window_minutes=30)
                    scheduled_actions.append(action)
        
        return {
            'status': 'success',
            'predictions_count': len(predictions),
            'critical_predictions': len([p for p in predictions if p.severity.value == 'critical']),
            'recommendations': len(recommendations),
            'scheduled_actions': scheduled_actions,
            'predictions': [p.to_dict() for p in predictions[:5]]  # Limit output
        }
        
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e),
            'message': 'Predictive health monitoring requires cost_monitor module'
        }


def workspace_curation(driver) -> Dict:
    """
    P3-T2: Intelligent Workspace Curation (6 hours, 1000 tokens)
    
    AI-powered workspace management:
    - Auto-name untitled pages
    - Suggest page consolidations
    - Auto-archive inactive content
    """
    print("  üé® Running workspace curation...")
    
    try:
        from ..workspace_curator import get_workspace_curator
        
        curator = get_workspace_curator(driver)
        
        # Run curation cycle in dry-run mode (review before applying)
        results = curator.run_curation_cycle(dry_run=True)
        
        # Apply auto-titling for high-confidence suggestions
        auto_titled = 0
        for page in results.get('untitled_pages', []):
            if page.get('confidence', 0) > 0.7 and page.get('suggested'):
                if curator.apply_title_suggestion(page['path'], page['suggested']):
                    auto_titled += 1
        
        return {
            'status': 'success',
            'untitled_pages_found': len(results.get('untitled_pages', [])),
            'auto_titled': auto_titled,
            'consolidation_suggestions': len(results.get('consolidation_suggestions', [])),
            'archive_candidates': len(results.get('archive_candidates', [])),
            'curation_stats': curator.get_curation_stats()
        }
        
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e),
            'message': 'Workspace curation requires workspace_curator module'
        }


def collaboration_orchestration(driver) -> Dict:
    """
    P3-T3: Agent Collaboration Orchestration (15 min, 500 tokens)
    
    Multi-agent task orchestration:
    - Monitor for collaboration opportunities
    - Spawn agent teams for complex tasks
    - Synthesize results from multiple agents
    """
    print("  ü§ù Running collaboration orchestration...")
    
    try:
        from ..agent_collaboration import get_collaboration_protocol
        
        protocol = get_collaboration_protocol(driver)
        
        # Check for pending tasks that need collaboration
        with driver.session() as session:
            result = session.run('''
                MATCH (t:Task)
                WHERE t.status = 'pending'
                  AND t.requires_collaboration = true
                  AND t.collaboration_id IS NULL
                RETURN t.id as id, t.title as title, t.description as description
                LIMIT 5
            ''')
            
            collaborations_initiated = 0
            for record in result:
                # Create collaboration for complex task
                task = protocol.create_from_template(
                    "complex_implementation",
                    record['title'],
                    record['description'],
                    context={"task_id": record['id']}
                )
                
                if task:
                    # Update task with collaboration reference
                    session.run('''
                        MATCH (t:Task {id: $task_id})
                        SET t.collaboration_id = $collab_id,
                            t.status = 'collaborating'
                    ''', task_id=record['id'], collab_id=task.id)
                    
                    collaborations_initiated += 1
        
        return {
            'status': 'success',
            'collaborations_initiated': collaborations_initiated,
            'active_collaborations': len(protocol.active_collaborations),
            'stats': protocol.get_collaboration_stats()
        }
        
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e),
            'message': 'Collaboration orchestration requires agent_collaboration module'
        }


# ============================================================================
# Task Execution - Executes Pending Tasks from Notion/Neo4j
# ============================================================================

def execute_pending_tasks(driver) -> Dict:
    """
    Execute pending tasks from Notion/Neo4j queue.
    
    This function runs in the OpenClaw tool context and can directly call
    sessions_spawn to create agent sessions for task execution.
    
    P0-TASK: Task execution (5 min, 500 tokens)
    """
    print("  ‚öôÔ∏è  Executing pending tasks...")
    
    import os
    import sys
    sys.path.insert(0, '/data/workspace/souls/main')
    
    result = {
        'status': 'success',
        'tasks_found': 0,
        'tasks_executed': 0,
        'tasks_failed': 0,
        'errors': [],
        'executed_tasks': []
    }
    
    try:
        # Check for pending tasks in Neo4j
        with driver.session() as session:
            # Find pending tasks with assignees
            pending_result = session.run("""
                MATCH (t:Task {status: 'pending'})
                WHERE t.assigned_to IS NOT NULL
                RETURN t.id as task_id, 
                       t.description as description,
                       t.assigned_to as assigned_to,
                       t.priority as priority,
                       t.notion_url as notion_url,
                       t.type as task_type
                ORDER BY 
                    CASE t.priority
                        WHEN 'critical' THEN 1
                        WHEN 'high' THEN 2
                        WHEN 'normal' THEN 3
                        ELSE 4
                    END
                LIMIT 5
            """)
            
            pending_tasks = [dict(record) for record in pending_result]
            result['tasks_found'] = len(pending_tasks)
            
            if not pending_tasks:
                print("    ‚ÑπÔ∏è  No pending tasks to execute")
                return result
            
            print(f"    Found {len(pending_tasks)} pending tasks")
            
            # Import TaskExecutor for task management (but not session spawning)
            try:
                from tools.task_executor import TaskExecutor
                
                # Create memory wrapper using existing driver
                class SimpleMemory:
                    def __init__(self, driver):
                        self.driver = driver
                    
                    def get_task(self, task_id):
                        with self.driver.session() as inner_session:
                            result = inner_session.run("""
                                MATCH (t:Task {id: $task_id})
                                RETURN t
                            """, task_id=task_id)
                            record = result.single()
                            return dict(record['t']) if record else None
                    
                    def update_task_status(self, task_id, status, **kwargs):
                        with self.driver.session() as inner_session:
                            inner_session.run("""
                                MATCH (t:Task {id: $task_id})
                                SET t.status = $status,
                                    t.updated_at = datetime()
                            """, task_id=task_id, status=status)
                
                memory = SimpleMemory(driver)
                
                # Create executor for task management
                executor = TaskExecutor(
                    memory=memory,
                    notion_integration=None
                )
                
                # Execute each pending task
                for task in pending_tasks:
                    try:
                        task_id = task.get('task_id')
                        assigned_to = task.get('assigned_to')
                        description = task.get('description', 'No description')
                        notion_url = task.get('notion_url')
                        task_type = task.get('task_type', 'task')
                        
                        if not task_id:
                            print(f"    ‚ö†Ô∏è  Skipping task with no ID")
                            continue
                        
                        print(f"    üöÄ Executing: {description[:50]}... (‚Üí {assigned_to})")
                        
                        # Mark as in_progress
                        session.run("""
                            MATCH (t:Task {id: $task_id})
                            SET t.status = 'in_progress',
                                t.claimed_by = $agent,
                                t.claimed_at = datetime()
                        """, task_id=task_id, agent=assigned_to)
                        
                        # Build task prompt for the agent
                        task_prompt = f"""You have been assigned a task from the Kurultai task queue.

TASK ID: {task_id}
TYPE: {task_type}
PRIORITY: {task.get('priority', 'normal')}

DESCRIPTION:
{description}

{f"NOTION REFERENCE: {notion_url}" if notion_url else ""}

Your mission:
1. Complete this task to the best of your ability
2. Use all available tools and resources
3. Report your results clearly
4. Update the task status when complete

Begin execution now.
"""
                        
                        # Spawn agent session using OpenClaw tool (available in this context)
                        try:
                            spawn_result = sessions_spawn(
                                task=task_prompt,
                                agent_id=assigned_to.lower(),
                                label=f"task-{task_id[:8]}",
                                timeout_seconds=300
                            )
                            
                            result['tasks_executed'] += 1
                            result['executed_tasks'].append({
                                'task_id': task_id,
                                'assigned_to': assigned_to,
                                'result': 'spawned',
                                'session': str(spawn_result) if spawn_result else None
                            })
                            
                            print(f"    ‚úÖ Task {task_id[:8]}... spawned to {assigned_to}")
                            
                        except Exception as spawn_error:
                            error_msg = f"Session spawn failed: {spawn_error}"
                            print(f"    ‚ùå {error_msg[:100]}")
                            
                            # Mark as failed
                            session.run("""
                                MATCH (t:Task {id: $task_id})
                                SET t.status = 'failed',
                                    t.error = $error
                            """, task_id=task_id, error=error_msg[:200])
                            
                            result['tasks_failed'] += 1
                            result['errors'].append(f"{task_id}: {error_msg}")
                        
                    except Exception as e:
                        error_msg = str(e)
                        print(f"    ‚ùå Failed to execute task: {error_msg[:100]}")
                        result['tasks_failed'] += 1
                        result['errors'].append(f"Task execution: {error_msg}")
                        
                        # Try to mark as failed
                        try:
                            if task_id:
                                session.run("""
                                    MATCH (t:Task {id: $task_id})
                                    SET t.status = 'failed',
                                        t.error = $error
                                """, task_id=task_id, error=error_msg[:200])
                        except:
                            pass
                
            except ImportError as e:
                print(f"    ‚ö†Ô∏è  TaskExecutor not available: {e}")
                result['errors'].append(f"Import error: {e}")
                
                # Fallback: Just log the tasks that need execution
                for task in pending_tasks:
                    print(f"    üìã Would execute: {task['description'][:40]}... ‚Üí {task['assigned_to']}")
                
    except Exception as e:
        print(f"    ‚ùå Error in execute_pending_tasks: {e}")
        result['status'] = 'error'
        result['errors'].append(str(e))
    
    print(f"    Summary: {result['tasks_executed']} executed, {result['tasks_failed']} failed")
    
    return result


# ============================================================================
# Agent Deliberation - Purpose-Driven Agent Communication
# ============================================================================

def trigger_deliberations(driver) -> Dict:
    """
    Check for task events that should trigger agent deliberations.
    
    Purpose-driven communication - agents talk when:
    - Research completes ‚Üí handoff to implementation
    - Task blocked ‚Üí problem-solving help
    - Complex deliverable ‚Üí quality review
    - Resource conflict ‚Üí coordination
    - Critical failure ‚Üí escalation
    
    P3-TASK: Trigger deliberations (5 min, 300 tokens)
    """
    print("  üß† Checking for deliberation triggers...")
    
    result = {
        'status': 'success',
        'deliberations_triggered': 0,
        'types': {},
        'timestamp': datetime.now().isoformat()
    }
    
    try:
        with driver.session() as session:
            # Check for recently completed research tasks (handoff trigger)
            completed_research = session.run("""
                MATCH (t:Task {status: 'completed'})
                WHERE t.type = 'research'
                  AND (t.deliberation_triggered IS NULL OR t.deliberation_triggered = false)
                  AND t.updated_at > datetime() - duration('PT10M')
                RETURN t.id as task_id,
                       t.assigned_to as researcher,
                       t.description as description,
                       t.results as results
                LIMIT 3
            """)
            
            for record in completed_research:
                # Mark as deliberation triggered
                session.run("""
                    MATCH (t:Task {id: $task_id})
                    SET t.deliberation_triggered = true,
                        t.deliberation_type = 'handoff',
                        t.deliberation_triggered_at = datetime()
                """, task_id=record['task_id'])
                
                print(f"    üîÑ Handoff: {record['researcher']} ‚Üí implementer for {record['description'][:40]}...")
                result['deliberations_triggered'] += 1
                result['types']['handoff'] = result['types'].get('handoff', 0) + 1
            
            # Check for blocked tasks (problem-solving trigger)
            blocked_tasks = session.run("""
                MATCH (t:Task {status: 'blocked'})
                WHERE (t.deliberation_triggered IS NULL OR t.deliberation_triggered = false)
                  AND t.blocker IS NOT NULL
                  AND t.updated_at > datetime() - duration('PT10M')
                RETURN t.id as task_id,
                       t.assigned_to as blocked_agent,
                       t.blocker as blocker,
                       t.description as description
                LIMIT 3
            """)
            
            for record in blocked_tasks:
                session.run("""
                    MATCH (t:Task {id: $task_id})
                    SET t.deliberation_triggered = true,
                        t.deliberation_type = 'problem_solving',
                        t.deliberation_triggered_at = datetime()
                """, task_id=record['task_id'])
                
                print(f"    ‚ö†Ô∏è  Blocked: {record['blocked_agent']} needs help with {record['blocker'][:40]}...")
                result['deliberations_triggered'] += 1
                result['types']['problem_solving'] = result['types'].get('problem_solving', 0) + 1
            
            # Check for complex completed tasks (review trigger)
            complex_completed = session.run("""
                MATCH (t:Task {status: 'completed'})
                WHERE t.complexity >= 7
                  AND (t.deliberation_triggered IS NULL OR t.deliberation_triggered = false)
                  AND t.updated_at > datetime() - duration('PT10M')
                RETURN t.id as task_id,
                       t.assigned_to as author,
                       t.description as description,
                       t.complexity as complexity
                LIMIT 2
            """)
            
            for record in complex_completed:
                session.run("""
                    MATCH (t:Task {id: $task_id})
                    SET t.deliberation_triggered = true,
                        t.deliberation_type = 'review',
                        t.deliberation_triggered_at = datetime()
                """, task_id=record['task_id'])
                
                print(f"    üëÅÔ∏è  Review: {record['author']} requests review (complexity {record['complexity']})")
                result['deliberations_triggered'] += 1
                result['types']['review'] = result['types'].get('review', 0) + 1
            
            # Check for critical failures (escalation trigger)
            critical_failures = session.run("""
                MATCH (t:Task {status: 'failed'})
                WHERE t.priority = 'critical'
                  AND (t.deliberation_triggered IS NULL OR t.deliberation_triggered = false)
                  AND t.updated_at > datetime() - duration('PT10M')
                RETURN t.id as task_id,
                       t.assigned_to as failed_agent,
                       t.error as error,
                       t.description as description
                LIMIT 1
            """)
            
            for record in critical_failures:
                session.run("""
                    MATCH (t:Task {id: $task_id})
                    SET t.deliberation_triggered = true,
                        t.deliberation_type = 'escalation',
                        t.deliberation_triggered_at = datetime()
                """, task_id=record['task_id'])
                
                print(f"    üö® ESCALATION: {record['failed_agent']} critical failure")
                result['deliberations_triggered'] += 1
                result['types']['escalation'] = result['types'].get('escalation', 0) + 1
        
        if result['deliberations_triggered'] == 0:
            print("    ‚ÑπÔ∏è  No deliberation triggers found")
        else:
            print(f"    ‚úÖ Triggered {result['deliberations_triggered']} deliberations")
            for dtype, count in result['types'].items():
                print(f"       - {dtype}: {count}")
    
    except Exception as e:
        print(f"    ‚ùå Error in trigger_deliberations: {e}")
        result['status'] = 'error'
        result['error'] = str(e)
    
    return result


# ============================================================================
# Task Registry
# ============================================================================

TASK_REGISTRY = {
    # √ñgedei (Ops)
    'health_check': {'fn': health_check, 'agent': '√∂gedei', 'freq': 5},
    'file_consistency': {'fn': file_consistency, 'agent': '√∂gedei', 'freq': 15},
    
    # Jochi (Analyst)
    'memory_curation_rapid': {'fn': memory_curation_rapid, 'agent': 'jochi', 'freq': 5},
    'mvs_scoring_pass': {'fn': mvs_scoring_pass, 'agent': 'jochi', 'freq': 15},
    'smoke_tests': {'fn': smoke_tests, 'agent': 'jochi', 'freq': 15},
    'full_tests': {'fn': full_tests, 'agent': 'jochi', 'freq': 60},
    'vector_dedup': {'fn': vector_dedup, 'agent': 'jochi', 'freq': 360},
    'deep_curation': {'fn': deep_curation, 'agent': 'jochi', 'freq': 360},
    
    # Chagatai (Writer)
    'reflection_consolidation': {'fn': reflection_consolidation, 'agent': 'chagatai', 'freq': 30},
    
    # M√∂ngke (Researcher)
    'knowledge_gap_analysis': {'fn': knowledge_gap_analysis, 'agent': 'm√∂ngke', 'freq': 60},
    'ordo_sacer_research': {'fn': ordo_sacer_research, 'agent': 'm√∂ngke', 'freq': 60},
    'ecosystem_intelligence': {'fn': ecosystem_intelligence, 'agent': 'm√∂ngke', 'freq': 60},
    
    # Kublai (Main)
    'status_synthesis': {'fn': status_synthesis, 'agent': 'kublai', 'freq': 5},
    'weekly_reflection': {'fn': weekly_reflection, 'agent': 'kublai', 'freq': 10080},
    
    # System
    'notion_sync': {'fn': notion_sync, 'agent': 'system', 'freq': 60},
    
    # Task Execution - NEW: Actually executes pending tasks
    'execute_pending_tasks': {'fn': execute_pending_tasks, 'agent': 'system', 'freq': 5},
    
    # Agent Deliberation - NEW: Purpose-driven agent communication
    'trigger_deliberations': {'fn': trigger_deliberations, 'agent': 'kublai', 'freq': 5},
    
    # PHASE 3: Kurultai v2.0 Enhanced Tasks
    'predictive_health_check': {'fn': predictive_health_check, 'agent': '√∂gedei', 'freq': 5},
    'workspace_curation': {'fn': workspace_curation, 'agent': 'jochi', 'freq': 360},
    'collaboration_orchestration': {'fn': collaboration_orchestration, 'agent': 'kublai', 'freq': 15},
}


def run_task(task_name: str, driver=None) -> Dict:
    """Run a single task by name."""
    if task_name not in TASK_REGISTRY:
        return {'status': 'error', 'error': f'Unknown task: {task_name}'}
    
    if driver is None:
        driver = get_driver()
    
    task_fn = TASK_REGISTRY[task_name]['fn']
    
    try:
        result = task_fn(driver)
        result['task'] = task_name
        result['timestamp'] = datetime.now().isoformat()
        return result
    except Exception as e:
        return {
            'status': 'error',
            'task': task_name,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }


async def register_all_tasks(hb):
    """Register all tasks with the heartbeat system."""
    from heartbeat_master import HeartbeatTask
    
    task_configs = {
        'health_check': {'tokens': 150, 'desc': 'System health monitoring'},
        'file_consistency': {'tokens': 200, 'desc': 'File integrity checks'},
        'memory_curation_rapid': {'tokens': 300, 'desc': 'Quick memory cleanup'},
        'mvs_scoring_pass': {'tokens': 400, 'desc': 'Memory value scoring'},
        'smoke_tests': {'tokens': 800, 'desc': 'Basic connectivity tests'},
        'full_tests': {'tokens': 1500, 'desc': 'Comprehensive test suite'},
        'vector_dedup': {'tokens': 800, 'desc': 'Duplicate detection'},
        'deep_curation': {'tokens': 2000, 'desc': 'Deep cleanup operations'},
        'reflection_consolidation': {'tokens': 500, 'desc': 'Merge related reflections'},
        'knowledge_gap_analysis': {'tokens': 600, 'desc': 'Identify knowledge gaps'},
        'ordo_sacer_research': {'tokens': 1200, 'desc': 'Specialized research'},
        'ecosystem_intelligence': {'tokens': 2000, 'desc': 'Ecosystem tracking'},
        'status_synthesis': {'tokens': 200, 'desc': 'Agent status aggregation'},
        'weekly_reflection': {'tokens': 1500, 'desc': 'Weekly system analysis'},
        'notion_sync': {'tokens': 800, 'desc': 'Notion bidirectional sync'},
        # Task Execution - CRITICAL: Executes pending tasks
        'execute_pending_tasks': {'tokens': 500, 'desc': 'Execute pending Notion/Neo4j tasks'},
        # Agent Deliberation - NEW: Purpose-driven agent communication
        'trigger_deliberations': {'tokens': 300, 'desc': 'Trigger agent deliberations on task events'},
        # PHASE 3 Tasks
        'predictive_health_check': {'tokens': 200, 'desc': 'Predictive health monitoring'},
        'workspace_curation': {'tokens': 1000, 'desc': 'AI-powered workspace curation'},
        'collaboration_orchestration': {'tokens': 500, 'desc': 'Multi-agent collaboration'},
    }
    
    for task_name, config in TASK_REGISTRY.items():
        tokens = task_configs.get(task_name, {}).get('tokens', 500)
        desc = task_configs.get(task_name, {}).get('desc', '')
        
        # Create async wrapper for sync function
        async def make_handler(fn):
            async def handler(driver):
                return fn(driver)
            return handler
        
        hb.register(HeartbeatTask(
            name=task_name,
            agent=config['agent'],
            frequency_minutes=config['freq'],
            max_tokens=tokens,
            handler=await make_handler(config['fn']),
            description=desc,
            enabled=True
        ))


def main():
    """CLI for running tasks."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Run agent background tasks')
    parser.add_argument('--task', '-t', required=True,
                       help='Task name to run',
                       choices=list(TASK_REGISTRY.keys()))
    parser.add_argument('--list', '-l', action='store_true',
                       help='List all available tasks')
    
    args = parser.parse_args()
    
    if args.list:
        print("Available tasks:")
        for name, config in TASK_REGISTRY.items():
            print(f"  {name:30} | {config['agent']:10} | {config['freq']}min")
        return
    
    print(f"Running task: {args.task}")
    result = run_task(args.task)
    print(json.dumps(result, indent=2))


if __name__ == '__main__':
    main()
